---
title: "Weight lifting prediction - Final Project"
author: "Luis Peraza"
date: "16 July 2016"
output:
  html_document:
    keep_md: yes
  word_document: default
---

##Synopsis

In this project, we will perform prediction for weight lifting styles. The aim of this assignment is to infer between the most common errors when doing a dumbell curl and classify/predict when these errors occur, in order to tell the perfomer that correction is needed.

A complete explanation of the experiment can be found here: http://groupware.les.inf.puc-rio.br/har. During the original experiments, the researchers invited six young healthy participants to do 10 repetitions of dumbell curls in 5 different ways; the correct one and four common errors. All the participants were supervised by an experienced trainer to perfectly reproduce the common errors and the correct exercise. The classification was as follows:

A: Exactly according to the specification, 
B: Throwing the elbows to the front,
C: Lifting the dumbbell only halfway, 
D: Lowering the dumbbell only halfway, 
E: Throwing the hips to the front,

Our final aim is to emulate the original results from this study and which were published in Velloso et al. (2013), who reported an accuracy of 99.9%

Let's start the analysis by loading the Weight Lifting Experiment dataset

```{r, message=FALSE, warning=FALSE}
library(caret)
library(dplyr)
training <- read.csv('pml-training.csv', header = TRUE)
```

Notice that I also loaded the libraries caret for modelling and dplyr for data frame manipulation.

Inspect the training dataset, just the first 20 variables,

```{r}
dim(training)
str(training, list.len=20)
```

First we see that it is a very large dataset, comprised by the raw measures from the accelerometers and statistics from these measures (stddev, kurtosis, var, etc) and which were estimated in windows of 2.5 secs according to the dataset description. Many of these are NAs and we won't use them in the prediction task. The assigment also asked to download the testing (for evaluation) dataset which doesn't have the statistic measures but it the has same number of variables.

```{r}
  evaldata <- read.csv('pml-testing.csv', header = TRUE)
```

Noticed then that we will need to choose from the training dataset the correct predictors for classification, i.e. ignoring the statistic variables. For this we can use the grep function to select variables or other approach, but in this case we can use the evaluation dataset to get the needed indices by not choosing the columns with NAs.

```{r}
  subtraining <- training[, colSums(is.na(evaldata)) == 0]
```

Hence, we have used in our advantage that the testing dataset has NA values in all the summarizing statistics,and we decided not to select those columns. We can inspect the subtraining dataset and see that it only has the raw sensor variables; a total of 52 predictors and the "classe" variable with the correct dumbbell curl class.

```{r}
  names(subtraining)
```

Here comes the exciting part. In the original research article by Velloso et al. (2013), which can be found in the link above and in this repo, the authors reported that they applied random forest classifiers and a bagging approach. At the beggining I was a little confused because random forest is already a bagged classifier. But it seems that the authors "rebagged" the random forest models. Hence, they bagged 10 random forest models where each forest was comprised of 10 trees, and the validation was perfomed with a 10-fold cross-validation per forest. At the moment, I ignore if the number 10 (which was chosen for all the parameters) has an impact in the performance, but this was the authors' approach.

Hence, I decided to emulate the authors' solution in my model and program the bagging myself. The bagging philosophy has three stages: 1) boostrap resampling, 2) Estimate several models, and 3) assemble the models using any approach, e.g majority vote.

Let's create our new subtraining and subtesting samples which will be independent,

```{r}
set.seed(4332)
inTrain <- createDataPartition(subtraining$classe, p=0.8, list=FALSE)
subtesting <-subtraining[-inTrain,]
subtraining <- subtraining[inTrain,]
```

Notice that we have allocated 80% of the samples for training and the 20% for testing. Now we can start with the bagging procedure. First we get the bootstrap indices,

```{r}
resampled <- createResample(subtraining$classe, times=10, list=FALSE)
```

The function createResample does a balanced boostraping for us (sampling with replacement), where all levels of the factor variable "classe"" will be represented. It is very important that classe is declared a as factor variable here, if it were numeric it may happen that some levels are missed in the new datasets. The variable resampled has the indices we need to create new bootstrapped datasets. 
Now we can create our 10 random forests with a for loop and save them in a list.

```{r, eval=FALSE}
#Create an empty model list where we will save the 10 random forest models
modlist <-list()
for(iter in 1:10){
  #Perform the resampling with replacement
  DataResampled <- subtraining[resampled[,iter],-(1:7)]
  #No partition will be created, since we are bootstraping the sample
  modrf <- train(classe ~ ., method = "rf", #We chose random forests as our model
                             data = DataResampled, #The bootstrapped dataset
                             importance = TRUE, #Tell me the variable importance 
                             trControl = trainControl(method = "cv", number = 10), #10-Fold CrossVal
                             ntree = 10) # VERY IMPORTANT! specify 10 trees in the forest
  modlist[[iter]] <- modrf #Save the model in the list
  print(iter) #Just count which model is being estimated
}
#Save the list of models (the lines above take time, better to save it)
save(modlist,file="modALLRF.RData")
```

We have decided to save the list of models because the estimation takes a lot time, depending on our processor. Load the models again

```{r}
load("modALLRF.RData")
```

Next, create a function for predictions and assembling of all models. We do these as follows:

```{r,message=FALSE, warning=FALSE}
MycombRFpredict<- function(modlist, newdata){
  #With sapply we will make predictions with all models in one line
  newpredictors <- sapply(modlist, function(x) predict(x,newdata))
  #Create a new dataframe with the classe variable and the 10 predictions from random forests
  assembledData <- data.frame(newpredictors, classe=newdata$classe)
  #Create a new prediction using lda for joining the classifiers. lda doesn't need any parameter
  combModFit <- train(classe ~.,method="lda",data=assembledData)
  combPred <- predict(combModFit,assembledData)
  combPred #Return the final prediction
}
```

Now we can evaluate the perfomance of the 10 random forests. First, we will call the function designed above and then use the confusionMatrix R function to estimate performance. We use the subtesting sample we create at the very beggining of our analysis and which is independent from the training set.

```{r,message=FALSE, warning=FALSE}
combinePred <- MycombRFpredict(modlist,subtesting)
confusionMatrix(combinePred, subtesting$classe)
```

For the testing set, we reached an accuracy of 99.4%

##For curiosity...

Do you remember that we asked to the train function to estimate variable importance? We can use it to visualize which variables are the most important for classification from any of the 10 random forest models in our list. For instance

```{r, fig.height= 6}
plot(varImp(modlist[[1]]),top = 15)
```

    Figure 1. Variable importance

In Figure 1 we have plotted the first 15 most important variables for the 1st model. These slighly vary between models and the order depends on the bootstraping stage of our analysis.

##Conclusion

For a completely independent test sample, subtesting, our combination of random forests reached an accuracy of 99.4% which is really good. Velloso et al. (2013) reported an accuracy of 99.9% which is awesome and we were not that far with our bagged model.

I hope you enjoyed my solution to this project. 

Cheers!

###Reference

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

Read more: http://groupware.les.inf.puc-rio.br/har#ixzz4EavvqJc1








